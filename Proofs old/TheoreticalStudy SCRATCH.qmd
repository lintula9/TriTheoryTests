---
title: "Theoretical study Quarto -document"
format: pdf
date: "2026-01-28"
editor: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Scratch.

Combining, we get a system of (matrix) equations

$$ 
\begin{cases} &(I-A \otimes A)^{-1} \text{vec}(\Psi) &=& \text{vec}(\Lambda \Lambda^T + \Omega)\\ &A &=& \delta I + (\Omega_{cross} - \delta \Omega)\Sigma_{t-1}^{-1} \end{cases} 
$$

substituting $A$ into the upper equation and further substituting
$A-\delta I$

$$ 
\begin{align*} (I-(\delta I + (\Omega_{cross} - \delta \Omega)\Sigma_{t-1}^{-1}) \otimes (\delta I + (\Omega_{cross} - \delta \Omega)\Sigma_{t-1}^{-1}))^{-1} \text{vec}(\Psi) &=  \\  (I-(\delta I + (\Omega_{cross} - \delta \Omega)\Sigma_{t-1}^{-1}) \otimes\delta I \\+ (\delta I + (\Omega_{cross} - \delta \Omega)\Sigma_{t-1}^{-1})\otimes ((\Omega_{cross} - \delta \Omega)\Sigma_{t-1}^{-1}))^{-1} \text{vec}(\Psi)&= \\ (I-\delta I\otimes \delta I + (\Omega_{cross} - \delta \Omega)\Sigma_{t-1}^{-1} \otimes\delta I + \delta I\otimes(\Omega_{cross} - \delta \Omega)\Sigma_{t-1}^{-1} \\+   (\Omega_{cross} - \delta \Omega)\Sigma_{t-1}^{-1}\otimes (\Omega_{cross} - \delta \Omega)\Sigma_{t-1}^{-1})^{-1} \text{vec}(\Psi)&= \\ (I-\delta^2I+(\Omega_{cross} - \delta \Omega)\Sigma_{t-1}^{-1} \otimes\delta I + \delta I\otimes(\Omega_{cross} - \delta \Omega)\Sigma_{t-1}^{-1} \\+(\Omega_{cross} - \delta \Omega)\Sigma_{t-1}^{-1}\otimes (\Omega_{cross} - \delta \Omega)\Sigma_{t-1}^{-1})^{-1} \text{vec}(\Psi)&= \\ (I-\delta^2I+(A- \delta I) \otimes\delta I + \delta I\otimes(A- \delta I) \\+(A- \delta I)\otimes (A- \delta I))^{-1} \text{vec}(\Psi)&= \\ (I-\delta^2I + A\otimes \delta I - \delta^2I + \delta I\otimes A + \delta^2I \\+ (A- \delta I)\otimes A + (A- \delta I)\otimes (- \delta I))^{-1} \text{vec}(\Psi)&= \\ (I-2\delta^2I+ \delta^2I + A\otimes \delta I + \delta I\otimes A  \\+ A\otimes A - \delta I\otimes A + A\otimes(- \delta I) - \delta I\otimes (- \delta I))^{-1} \text{vec}(\Psi)&= \\ (I-2\delta^2I+ \delta^2I   + A\otimes A   - \delta I\otimes (- \delta I))^{-1} \text{vec}(\Psi)&= \\ (I-2\delta^2I+ 2\delta^2I   + A\otimes A  )^{-1} \text{vec}(\Psi)&= (I-A\otimes A  )^{-1} \text{vec}(\Psi) \end{align*} 
$$ The analysis leads to no easy contradiction. (Chat GPT 3.5 confirmed
that the above analysis is correct. Hence, we're uncertain if it is
correct.)

The vectorized covariance matrix can be solved to equal $$
\begin{align*}
\text{vec}(E[X_tX_t^T])&=\text{vec}(\Sigma_{VAR(1)}) \\
&=\text{vec}(AE[X_{t-1}X_{t-1}^T]A^T + \Psi) \\
&=\text{vec}(AE[X_{t-1}X_{t-1}^T]A^T) + \text{vec}(\Psi) \\
&=\text{vec}(A\otimes A)E[X_{t-1}X_{t-1}^T]+\text{vec}(\Psi) \\
\Longrightarrow\\
\text{vec}(E[X_tX_t^T])&=\text{vec}(A\otimes A)\text{vec}(E[X_{t-1}X_{t-1}^T])+\text{vec}(\Psi)&\Rightarrow\\
\text{vec}(E[X_tX_t^T])&=\text{vec}(A\otimes A)\text{vec}(E[X_{t-1}X_{t-1}^T])+\text{vec}(\Psi)&\Rightarrow\\
I&=\text{vec}(A\otimes A)+\text{vec}(\Psi)\text{vec}(E[X_tX_t^T])^{-1}&\Rightarrow\\
I-\text{vec}(A\otimes A) &= \text{vec}(\Psi)\text{vec}(E[X_tX_t^T])^{-1}&\Rightarrow\\
\text{vec}(E[X_tX_t^T])&= (I-A \otimes A)^{-1} \text{vec}(\Psi)
\end{align*}
$$

Using that we assumed the innovations to be independent of each other we
can rewrite the right hand side so that

$$
\begin{align*}
\text{vec}(\Psi)&=(\psi_{[1,1]},\psi_{[1,2]},...,\psi_{[2,2]},\psi_{[2,3]},...,\psi_{[K,K]})
\\
&=(\psi_{[1,1]},0,...,\psi_{[2,2]},0,...,\psi_{[K,K]}),\\
\end{align*}
$$ and $I-A \otimes A$ is $$
\begin{align*}
\begin{array}
&&1-a_{1,1}a_{1,1}&a_{1,1}a_{1,2}&...&a_{1,1}a_{1,K}&...&...&a_{1,K}a_{1,1}&a_{1,K}a_{1,2}&...&a_{1,K}a_{1,K}&\\
&a_{1,1}a_{2,1}&1-a_{1,1}a_{2,2}&...&a_{1,1}a_{2,K}&...&...&a_{1,K}a_{2,1}&a_{2,K}a_{2,2}&...&a_{2,K}a_{2,K}&\\
&\vdots&\vdots&\ddots&\vdots&&&\vdots&\vdots&\ddots&\vdots&\\
&a_{1,1}a_{K,1}&a_{1,1}a_{K,2}&...&1-a_{1,1}a_{K,K}&...&...&a_{1,K}a_{K,1}&a_{2,K}a_{K,2}&...&a_{2,K}a_{K,K}&\\
&\vdots&\vdots&&\vdots&\ddots&&\vdots&\vdots&&\vdots&\\
&\vdots&\vdots&&\vdots&&\ddots&\vdots&\vdots&&\vdots&\\
&a_{K,1}a_{1,1}&a_{K,1}a_{1,2}&...&a_{K,1}a_{1,K}&...&...&1-a_{K,K}a_{1,1}&a_{K,K}a_{1,2}&...&a_{K,K}a_{1,K}&\\
&a_{K,1}a_{2,1}&a_{K,1}a_{2,2}&...&a_{K,1}a_{2,K}&...&...&a_{K,K}a_{2,1}&1-a_{K,K}a_{2,2}&...&a_{K,K}a_{2,K}&\\
&\vdots&\vdots&\ddots&\vdots&&&\vdots&\vdots&\ddots&\vdots&\\
&a_{K,1}a_{K,1}&a_{K,1}a_{K,2}&...&a_{K,1}a_{K,K}&...&...&a_{K,K}a_{K,1}&a_{K,K}a_{K,2}&...&1-a_{K,K}a_{K,K}&
\end{array}
\end{align*}
$$

Let $c_{[i,j]}$ be and element of $\Sigma$. $\text{vec}(\Sigma)$ then is
$(c_{[1,1]},c_{[2,1]},...,c_{[K,1]},c_{[1,2]},...,c_{[K,2]},...,c_{[K,K]})$.
$(I-A\otimes A) \text{vec}(\Sigma)$ then becomes

the above might be omitted, since it maybe is not going anywhere. It
does lead to an equation for each innovations gamma_i,j, which might be
not relevant.

As the scenario where two subsequent measurement points are observed is
possibly the most common one, we'll keep the above discussion in place
for now. On the other hand, it is of interest to analyze what happens
when multiple subsequent time points are included. This is perhaps less
common in measurement invariance literature, but more common in VAR
literature.

We have (proven below) that as the distance in time between two time
points $\Delta t=(t_i-t_j)\to\infty, \:i>j$ samples from VAR(1) model
generated data at those two time points have 0 covariance. This would
make the asymptotic $2K\times2K$ perfectly explained by a measurement
invariance model, because the main diagonal covariance matrices
$\Sigma_{t-1}=\Sigma_t$ are perfectly explained by a common factor model
by assumption, and the off diagonal matrices are 0, which is allowed in
a strict LMI model (no cross-covariance between the observed time points
and 0 regression coefficient for the latent variable). Again, we are
assuming that VAR(1) model generated $\Sigma$ perfectly compatible with
s-LMI. This is a less practically meaningful case arguably since
observed data with 0 across time point covariance is not common. This
also does prove that a VAR(1) process could in some sense be the true
data generating process even if no across time point covariance is
observed (no lagged effects are estimated) if one were to claim that
$\Delta t$ is very small: We're just not observing time points close
enough to each other to see the VAR(1).

Nevertheless - considering that the above scenario is not a typical one
in psychopathology research - we can attempt to generalize the above
results concerning the $2K\times2K$ matrix to an $TK\times TK$ matrix
where we have $T$ measurement of $K$ symptoms over occasions at constant
time intervals. Brief notes are made as we move on and a summary at the
end.

Proof of the asymptotic 0 across time point covariance. On a brief note
we can further decompose the above equation, using the power method of
eigenvalues, into

$$ PD^{(T)}P^{-1}\Sigma_{VAR(1)} $$

where $D$ is a diagonal matrix of eigenvalues of $A$, $P$ is and
orthonormal matrix of eigenvectors of $A$ as columns. From this
decomposition we directly see the above mentioned asymptotic property
that as distance in time between time points increases, $D$ is raised to
a larger power and decreases eventually to the zero matrix. This is
because eigenvalues of (stationary) $A$ are less than one, meaning that
all diagonal elements of $\text{diag}(D):|d_{ii}|<1$ as well, and hence
the matrix power converges to $0$. This means that a true data
generating stationary VAR(1) model should produce across time point
covariances that reduce to zero as distance in time between any two time
points increases (practically speaking perhaps as years pass). Such
observations might be sparse in psychopathology literature and it is
accepted in the literature that even if any autoregressive model would
be the true data generating model, it would unlikely be stationary over
lengthier time periods (e.g., years again). This property of stationary
VAR(1) does - as we have -- (as Jaakko suggested) restrain the analysis
to a psychopathological state, but this contemplation is omitted here.

S Where the two covariances above must be the same - i.e., the
covariance matrix is symmetric. This means that every VAR(1) process
implies that covariance of observations from two subsequent time points
$t, t-1$ is $$
Cov((X_{t-1},X_{t}),(X_{t-1},X_{t})) = 
\begin{pmatrix} 
  \Sigma_{t-1} & A\Sigma_{t-1} \\
  \Sigma_{t-1}^TA^T & \Sigma_t
\end{pmatrix} $$ Where the above covariance matrix is the $2K\times 2K$
covariance matrix of the observed data from the two time points. In
addition, stationarity directly implies $\Sigma_t=\Sigma_{t-1}$. (For
now, notation with sub-index i will be kept for clarity as it is. s-LMI
is not necessarily imply stationary, so confusion might be avoided.)

For some off diagonal element of the across time points covariance
matrix, we have covariance between $X_{i,t},X_{j,t+\Delta t}$ $i\ne j$
at different time points is $$
\begin{cases}
\delta^{\Delta t} \lambda_{i}\lambda_{j}=\sigma_{ \{i,j,t,t+\Delta t\} } \\
a_t=\sigma_{ \{i,j,t,t+\Delta t\}}
\end{cases}
$$ we get a closed form for $\delta$ $$
\begin{cases}
\delta = \frac{ a_{11}\sigma_{1,2}+a_{12}\sigma_{2,2}+...+a_{1K}\sigma_{K,2}} {\lambda_1\lambda_2} \\
\delta = \frac{ a_{11}\sigma_{1,3}+a_{12}\sigma_{2,3}+...+a_{1K}\sigma_{K,3}} {\lambda_1\lambda_3} \\
\delta = \frac{ a_{11}\sigma_{1,4}+a_{12}\sigma_{2,4}+...+a_{1K}\sigma_{K,4}} {\lambda_1\lambda_4} \\
\dots
\end{cases}
$$

and so forth. This means that

$$
\frac{ a_{11}\sigma_{1,2}+a_{12}\sigma_{2,2}+...+a_{1K}\sigma_{K,2}} {\lambda_1\lambda_2} =\frac{ a_{11}\sigma_{1,3}+a_{12}\sigma_{2,3}+...+a_{1K}\sigma_{K,3}} {\lambda_1\lambda_3} \\
\frac{ a_{11}\sigma_{1,2}+a_{12}\sigma_{2,2}+...+a_{1K}\sigma_{K,2}} {\lambda_2} =\frac{ a_{11}\sigma_{1,3}+a_{12}\sigma_{2,3}+...+a_{1K}\sigma_{K,3}} {\lambda_3}\\
A_{[1,.]}\Sigma_{[.,2]}\frac{\lambda_3}{\lambda_2}
$$
