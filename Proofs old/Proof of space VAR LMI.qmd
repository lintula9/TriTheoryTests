---
title: "Supplementary Material: Proofs"
format: docx
type: document
editor: visual
---

### S-LMI Covariance

s-LMI with 1 common factor decomposes $\Sigma_{t-1}$ into following
$$\Sigma=\Lambda\Lambda^T+\Omega_{t-1}$$where by definition of s-LMI
$\Omega_{t-1}$ is diagonal and and $\Lambda$ is a $K\times1$ column
vector of factor loadings constant over time. We also need the
covariance of the common factor at both time points. Let $\delta$ be the
latent regression coefficient which links the common factor to itself at
a previous time point such that $\eta_t=\delta\eta_{t-1}+\psi_t$, where
$\psi_t$ is independent random term ('innovation', 'error',
'disturbance') with $E[\psi_t]=0$. Assuming standardized common factor
such that $E[\eta_{t-1}]=0,\:Var(\eta_{t-1})=1$ covariance of the common
factor at two subsequent time points is

$$
\begin{aligned} 
\text{Cov}(\eta_{t-1},\eta_t)= E[\eta_{t-1}\eta_t]-E[\eta_{t-1}]E[\eta_t]&=\\ E[\eta_{t-1}(\delta\eta_{t-1}+\psi_t)]&=\\ E[\delta\eta_{t-1}^2+\eta_{t-1}\psi_t]&=\\ \delta Var(\eta_{t-1})&= \delta 
\end{aligned}
$$ Now lets look at the $2K\times 2K$ covariance matrix from the
perspective of strict LMI. A s-LMI model imposes that $$
\text{Cov}((X_{t},X_{t-1}),(X_{t},X_{t-1}))= \begin{pmatrix}    \Lambda & 0
\\   0 & \Lambda \end{pmatrix}  \begin{pmatrix}   1 & \delta 
\\   \delta & \delta+Var(\psi_t) \end{pmatrix} \begin{pmatrix}    \Lambda^T & 0
\\   0 & \Lambda^T \end{pmatrix} + \begin{pmatrix}   \Omega_{t-1} & \Omega_{across} 
\\   \Omega_{across}^T & \Omega_t \end{pmatrix} 
$$ where $$
\begin{pmatrix}    \Lambda & 0\\   0 & \Lambda \end{pmatrix}
$$ is a block matrix that sandwiches the $2\times2$ covariance matrix of
the common factor at both time points. $$
\begin{aligned}
&\begin{pmatrix}    \Lambda & 0\\   0 & \Lambda \end{pmatrix} \begin{pmatrix}   1 & \delta \\   \delta & \delta+Var(\psi_t) \end{pmatrix} \begin{pmatrix}    \Lambda^T & 0\\   0 & \Lambda^T \end{pmatrix}+ \begin{pmatrix}   \Omega_{t-1} & \Omega_{across} \\   \Omega_{across}^T & \Omega_t \end{pmatrix} =\\
&\begin{pmatrix}    \Lambda & 0\\   0 & \Lambda \end{pmatrix} \begin{pmatrix}    \Lambda^T & \delta\Lambda^T\\   \Lambda^T\delta & (\delta+Var(\psi_t)\Lambda^T \end{pmatrix} + 
\begin{pmatrix}   
\Omega_{t-1} & \Omega_{across} \\   
\Omega_{across}^T & \Omega_t \end{pmatrix}=\\
& \begin{pmatrix}    
\Lambda\Lambda^T + \Omega_{t-1}& \Lambda\Lambda^T\delta + \Omega_{across}\\   
\Lambda\Lambda^T\delta + \Omega_{across}^T & \Lambda\Lambda^T(\delta+Var(\psi_t)) + \Omega_t 
\end{pmatrix} 
\end{aligned}
$$ From the above we see that the strict LMI can only be compatible with
any process with stationary covariance, if
$\delta+Var(\psi_t)=1\Rightarrow1-\delta=Var(\psi_t)$ (assuming
$\Lambda$ is non-zero). (When fitting a s-LMI model this is allowed.) We
also see that s-LMI is compatible with non-stationary processes where
the covariance is proportional to $\delta+Var(\psi_t)$ aligning with
previous theoretical analysis where covariance increased over time in a
LMI preserving model.

A brief note on notation: We'll be using simply $\Omega$ for the s-LMI
residual covariance, since residual covariance is assumed invariant over
time $\Omega_{t-1}=\Omega_{t}=\Omega_{t+1}=…=\Omega$ .

Using the above auxiliary results we can move to analyse the null
hypothesis (hypotheses) of no difference between VAR(1) and s-LMI.

### Proof for $cJ_k+dI$ spanning (matrix) vector space

-   Refinements are necessary, but central ideas should be visible.
-   Some proofs might be good to show here and use this as a supplement.

Let $0<c,d\in\mathbb{R}$. Let $J_k$ be a $k$ dimensional matrix of ones, the subindex of which will be omitted in the following. Let $I_k$ be $k$ dimensional identity matrix, similarly omitting the subindex. We will first show that the space which these two matrices span contains a set of drift matrices $A\in span(J,I)$ that produce covariance matrices perfectly also explained by S-LMI model.

Let $A=cJ+dI$. Let $\Sigma\in\mathbb{R}^{k\times k}$ be the positive semidefinite hermitian covariance matrix of random variables $X$ observed at different time points $t\in\mathbb{N}$. We will omit any special cases that might occur at small $k\leq3$ due to the factor model being just identifiable at $k=3$. The cross-covariance is $A\Sigma=(cJ+dI)\Sigma=cJ\Sigma+d\Sigma$. Note that $J\Sigma$ is effectively summing over the covariances for each variable producing then $k$ different values, one for each column of $\Sigma$, such that

$$
cJ\Sigma=
\begin{pmatrix} 
 c\sum_{i=1}^k\sigma_{i1}&\dots& c\sum_{i=1}^k\sigma_{ik} \\
 \vdots&\ddots&\vdots \\
 c\sum_{i=1}^k\sigma_{i1}&\dots& c\sum_{i=1}^k\sigma_{ik}
<<<<<<< HEAD
\end{pmatrix} = 
\begin{pmatrix} 
 a&\dots& b \\
 \vdots&\ddots&\vdots \\
 a&\dots& b
\end{pmatrix}
$$where $a\in\mathbb{R}$. Clearly $d\Sigma$ is a scalar multiple of the covariance.

Define innovations contained in $\Gamma\in\mathbb{R}^{k\times1}$ to have $E[\Gamma] = 0$ and $E[\Gamma\Gamma^T]=0$ so that the covariance of the innovations $\Psi$ is diagonal. We also need that the innovation variances are the same $\psi_1=\psi_2=…=\psi_k$. Since $A$ completely now defines covariance of $X$, we have that all variables must have identical covariance $\sigma_{12}=\sigma_{13}=\sigma_{23}…=\sigma_{ij}$ for all $i\ne j$. This is because $A$ defines identical cross-lagged autoregression coefficients for all $x\in X$. Note that $A$ also has identical autoregression coefficients and hence the variances are also identical for all $x$. This means that
=======
\end{pmatrix} 
$$
clearly $d\Sigma$ is just a scalar multiple of the covariance. Define innovations contained in $\Gamma\in\mathbb{R}^{K\times1}$ to have mean of 0 $E[\Gamma] = 0$ and are independent $E[\Gamma\Gamma^T]=0$. We also have that, since $A$ completely now defines covariances of $X$, that all variables have identical covariance $\sigma_{12}=\sigma_{13}=\sigma_{23}…=\sigma_{ij}$ for all $i\ne j$. This is because $A$ defines identical cross-lagged autoregression coefficients for all $x\in X$. Note that $A$ also has identical autoregression coefficients and hence the variances are also identical for all $x$. This means that
>>>>>>> a8ea11d3a17c795560904889855c451657b8bbc7

$$
cJ\Sigma=\begin{pmatrix} 
 a&\dots& a \\
 \vdots&\ddots&\vdots \\
 a&\dots& a \end{pmatrix}
\in span(J)
$$
where $a\in\mathbb{R}$. $d\Sigma,\Sigma\in span(J,I)$ (since $\Sigma$ only has two different values one for covariance and variance) hence also

$$
A\Sigma=(cJ+dI)\Sigma\in span(J,I)
$$

Also see that

$$
AA=(cJ+dI)(cJ+dI)=c^2kJ+dcJ+dcJ+d^2I=J\underbrace{(c^2k+2dc)}_{\in\mathbb{R}}+\underset{\in\mathbb{R}}{d^2}I\in span(J,I)
$$

where we can see that $AA,A$ have coordinates in the space $span(J,I)$, derivable from the equation above. It follows that $A^{\Delta}\Sigma\in span(J,I)$ which means that all cross covariances for any change in time $\Delta=t_T-t_0$ are also in this space.

<<<<<<< HEAD
Now, since we have that all covariances are the same, then for the S-LMI model with $\text{Var}(\eta)=1,E[\eta]=0$ all factor loadings must be the same so that$\lambda_1=\lambda_2=…=\lambda_k=\lambda:\text{Cov}(X_i,X_k)=\lambda^2,i\ne j$ . The residual cross covariances must also be the same, at some $\Delta$, as seen above. (By this point the practical constraint is evident, but will be discussed later, elsewhere. - Sakari) Now we have that the S-LMI imposed cross covariance is
=======
Now, since we have that all covariances are the same, then for the S-LMI model with $\text{Var}(\eta)=1,E[\eta]=0$ all factor loadings must be the same so that$\lambda_1=\lambda_2=…=\lambda_k=\lambda:\text{Cov}(X_i,X_j)=\lambda^2,i\ne j$ . The residual cross covariances must also be the same given some $\Delta$, as seen above. (By this point the practical constraint is evident, but will be discussed later, elsewhere. - Sakari) Now we have that the S-LMI imposed cross covariance is
>>>>>>> a8ea11d3a17c795560904889855c451657b8bbc7

$$
\Lambda\Lambda^T \delta_{\Delta}+\Omega_\Delta= cJ+dI\in span(J,I)
$$

<<<<<<< HEAD
where $\delta\in\mathbb{R}$ is the regression coefficient for subsequent $\eta_1,\eta_2$ and the respective product is for multiple subsequent regressions. $\delta_{\Delta}=\prod_{i=t}^{T}\delta_i$ is the product of regression coefficients of the latent variable to itsefl at previous timepoints. $\Omega_\Delta$ is a diagonal matix of residual covariances, all of which are the same.

Another, more general, criterion for indistinguishability that the subspace of positive semi-definite symmetric matrices $(\Sigma=\Lambda\Lambda^T+\Omega)\in\mathbb{R}^{K\times K}$ must be invariant under $A$. This is because $\Lambda\Lambda^T\delta + \Omega_{\Delta}$ is always positive semi-definite symmetric.

-   Correlation cannot separate VAR from S-LMI as well as covariance. This is because the innovations will have identity correlation, which is required for indistinguishability. Using covariance, innovations might be estimated as non-equivalent.

-   



### No measurement error scenario

If we look at a theoretical scenario where we observe psychopathology without measurement error, and
assume that there is no unique variance ($\Omega=0$), then $$
\begin{aligned}
&A^{(\Delta t)}\Sigma=\Lambda\Lambda^T \delta^{\Delta t}+ \underbrace{\Omega_{\Delta t}}_{=0}&\\     
\Rightarrow&A^{(\Delta t)}\Sigma=\underbrace {\Lambda\Lambda^T}_{\text{By assumption: }\Sigma-\Omega=\Sigma} \delta^{\Delta t}&\\
\Rightarrow&A^{(\Delta t)}\Sigma=\delta^{\Delta t}\Sigma&\\
\end{aligned}
$$ 
We see that $A$ must hence be scalar multiple of the identity matrix
(or a scalar). Since $\Sigma =\Lambda\Lambda^T$ this means, that $\Lambda$ must be an eigenvector of $A$.
=======
where $\delta\in\mathbb{R}$ is the regression coefficient for subsequent $\eta_1,\eta_2$ and the respective product is for multiple subsequent regressions: $\delta_{\Delta}=\prod_{i=t_0}^{T}\delta_i$.

#### General criterion for indistinguishability

Assumptions

$$
\begin{aligned}
|\lambda_A|&<1\Rightarrow \det A <1 &&& \text{(VAR is stationary.)} \\
\det\Omega, \det\Psi&\ne0 &&& \text{(Diagonal residuals and innovations are non-zero.)} \\
\text{rank }\Lambda\Lambda^T&=1&&& \text{(One common factor.)}
\end{aligned}
$$

We have a set of conditions, and their implications, which must be satisfied, if indistinguishability holds:

$$
\begin{aligned}
\Sigma&=A\Sigma A^T+\Psi &&&\text{(Within time point VAR covariance is common factor decomposable.)} \\
&=\Lambda\Lambda^T+\Omega \\&=
A(\Lambda\Lambda^T+\Omega)A^T + \Psi \\ 
\\
\Sigma_{\Delta}&=A^{\Delta}\Sigma &&&\text{(VAR cross-covariance is LMI decomposable.)} \\\ &=
\Lambda\Lambda^T\delta_{\Delta}+\Omega_{\Delta} \\ &=
A^{\Delta}(\Lambda\Lambda^T+\Omega) \\ \forall\Delta&\in1,2,3,... \\ \forall \delta_{\Delta}&\in\mathbb{R} \\ \\
\Sigma_{\Delta}^T    &=(\Lambda\Lambda^T\delta_{\Delta})^T+\Omega_{\Delta}^T   &&&\text{(Cross-covariance is always symmetric.)} \\
&=
\Lambda\Lambda^T\delta_{\Delta}+\Omega_{\Delta} \\ &=
\Sigma_{\Delta} 
\\ \\
\Sigma_{\Delta}&=A\Sigma_{\Delta-1}\\&=(A\Sigma_{\Delta-1})^T\\&=\Sigma_{\Delta-1}A^T
\end{aligned}  
$$
>>>>>>> a8ea11d3a17c795560904889855c451657b8bbc7
