---
title: "Supplementary Material: Proofs"
format: docx
type: document
editor: visual
---

### S-LMI Covariance

s-LMI with 1 common factor decomposes $\Sigma_{t-1}$ into following $$\Sigma=\Lambda\Lambda^T+\Omega_{t-1}$$where by definition of s-LMI $\Omega_{t-1}$ is diagonal and and $\Lambda$ is a $K\times1$ column vector of factor loadings constant over time. We also need the covariance of the common factor at both time points. Let $\delta$ be the latent regression coefficient which links the common factor to itself at a previous time point such that $\eta_t=\delta\eta_{t}+\psi_t$, where $\psi_t$ is independent random term ('innovation', 'error', 'disturbance') with $E[\psi_t]=0$. Assuming standardized common factor such that $E[\eta_{t}]=0,\:Var(\eta_{t})=1$ covariance of the common factor at two subsequent time points is

$$
\begin{aligned} 
\text{Cov}(\eta_{t},\eta_{t+1})= E[\eta_{t}\eta_{t+1}]-E[\eta_{t}]E[\eta_{t+1}]&=\\ E[\eta_{t}(\delta\eta_{t}+\psi_t)]&=\\ E[\delta\eta_{t}^2+\eta_{t}\psi_t]&=\\ \delta Var(\eta_{t})&= \delta 
\end{aligned}
$$Now lets look at the $2K\times 2K$ covariance matrix from the perspective of strict LMI. Using $\Delta$ as the (integer) time difference between two time-points, an s-LMI model imposes that $$
\text{Cov}((X_{t},X_{t-1}),(X_{t},X_{t-1}))= 
\begin{pmatrix}    
\Lambda & 0
\\   0 & \Lambda \end{pmatrix}  \begin{pmatrix}   I & \delta 
\\   \delta & \delta+Var(\psi_t) \end{pmatrix} \begin{pmatrix}    \Lambda^T & 0
\\   0 & \Lambda^T \end{pmatrix} + \begin{pmatrix}   \Omega_1 & \Omega_{(\Delta = 1)} 
\\   \Omega_{(\Delta = 1)} & \Omega_2 \end{pmatrix} 
$$ where $$
\begin{pmatrix}    \Lambda & 0\\   0 & \Lambda \end{pmatrix}
$$ is a block matrix that sandwiches the $2\times2$ covariance matrix of the common factor at both time points. $$
\begin{aligned}
\begin{pmatrix}    
\Lambda & 0\\   0 & \Lambda \end{pmatrix} \begin{pmatrix}   1 & \delta \\   \delta & \delta+Var(\psi_t) \end{pmatrix} \begin{pmatrix}    \Lambda^T & 0\\   0 & \Lambda^T \end{pmatrix}+ 
\begin{pmatrix}   \Omega_{t-1} & \Omega_{(\Delta=1)} \\   \Omega_{(\Delta=1)}^T & \Omega_t \end{pmatrix} =\\
\begin{pmatrix}    \Lambda & 0\\   0 & \Lambda \end{pmatrix} \begin{pmatrix}    \Lambda^T & \delta\Lambda^T\\   \Lambda^T\delta & (\delta+Var(\psi_t)\Lambda^T \end{pmatrix} + 
\begin{pmatrix}   
\Omega_{t-1} & \Omega_{(\Delta=1)} \\   
\Omega_{(\Delta=1)}^T & \Omega_t \end{pmatrix}=\\
\begin{pmatrix}    
\Lambda\Lambda^T + \Omega_{t-1}& \Lambda\Lambda^T\delta + \Omega_{(\Delta=1)}\\   
\Lambda\Lambda^T\delta + \Omega_{(\Delta=1)}^T & \Lambda\Lambda^T(\delta+Var(\psi_t)) + \Omega_t 
\end{pmatrix} 
\end{aligned}
$$From the above we see that the strict LMI can only be compatible with any process with stationary covariance, if $\delta+Var(\psi_t)=1\Rightarrow1-\delta=Var(\psi_t)$ (assuming $\Lambda$ is non-zero). We also see that S-LMI is compatible with non-stationary (here equivalently, 'time-varying') processes where the covariance is proportional to $\delta+Var(\psi_t)$ aligning with previous theoretical analysis where covariance increased over time in a LMI preserving model.

A brief note on notation: We'll be using simply $\Omega_{\Delta}$ for the S-LMI residual covariance across time, and assume residual covariance is invariant over time $\Omega_{t-1}=\Omega_{t}=\Omega_{t+1}=…=\Omega$

### Proof for $cJ_k+dI$ spanning (matrix) vector space

-   Refinements are necessary, but central ideas should be visible.
-   Some proofs might be good to show here and use this as a supplement.

Let $0<c,d\in\mathbb{R}$. Let $J_k$ be a $k$ dimensional matrix of ones, the subindex of which will be omitted in the following. Let $I_k$ be $k$ dimensional identity matrix, similarly omitting the subindex in the following. We will first show that the space which these two matrices span contains a set of drift matrices $A\in span(J,I)$ that produce covariance matrices perfectly also explained by S-LMI model.

Let $A=cJ+dI$. Let $\Sigma\in\mathbb{R}^{k\times k}$ be the positive semidefinite hermitian covariance matrix of random variables $X$ observed at different time points $t\in\mathbb{N}$. We will omit any special cases that might occur at small $k\leq3$ due to the factor model being just identifiable at $k=3$. The cross-covariance is $A\Sigma=(cJ+dI)\Sigma=cJ\Sigma+d\Sigma$. Note that $J\Sigma$ is effectively summing over the covariances for each variable producing then $k$ different values, one for each column of $\Sigma$, such that

$$
cJ\Sigma=
\begin{pmatrix} 
 c\sum_{i=1}^k\sigma_{i1}&\dots& c\sum_{i=1}^k\sigma_{ik} \\
 \vdots&\ddots&\vdots \\
 c\sum_{i=1}^k\sigma_{i1}&\dots& c\sum_{i=1}^k\sigma_{ik}
\end{pmatrix} = 
\begin{pmatrix} 
 a&\dots& b \\
 \vdots&\ddots&\vdots \\
 a&\dots& b
\end{pmatrix}
$$where $a\in\mathbb{R}$. Clearly $d\Sigma$ is a scalar multiple of the covariance.

Define innovations contained in $\Gamma\in\mathbb{R}^{k\times1}$ to have $E[\Gamma] = 0$ and $E[\Gamma\Gamma^T]=0$ so that the covariance of the innovations $\Psi$ is diagonal. We also need that the innovation variances are the same $\psi_1=\psi_2=…=\psi_k$. Since $A$ completely now defines covariance of $X$, we have that all variables must have identical covariance $\sigma_{12}=\sigma_{13}=\sigma_{23}…=\sigma_{ij}$ for all $i\ne j$. This is because $A$ defines identical cross-lagged autoregression coefficients for all $x\in X$. Note that $A$ also has identical autoregression coefficients and hence the variances are also identical for all $x$. This means that

$$
cJ\Sigma=\begin{pmatrix} 
 a&\dots& a \\
 \vdots&\ddots&\vdots \\
 a&\dots& a \end{pmatrix}
\in span(J)
$$

and $d\Sigma,\Sigma\in span(J,I)$ hence also

$$
A\Sigma=(cJ+dI)\Sigma\in span(J,I)
$$

Also see that

$$
AA=(cJ+dI)(cJ+dI)=c^2kJ+dcJ+dcJ+d^2I=J\underbrace{(c^2k+2dc)}_{\in\mathbb{R}}+\underset{\in\mathbb{R}}{d^2}I\in span(J,I)
$$

where we can see that $AA,A$ have coordinates in the space $span(J,I)$, derivable from the equation above. Using induction this can be proven for $A\times A \times...\times A\Sigma\in span(J,I)$ which means that all cross covariances for any change in time $\Delta=t_T-t_0$ are also in this space.

Now, since we have that all covariances are the same, then for the S-LMI model with $\text{Var}(\eta)=1,E[\eta]=0$ all factor loadings must be the same so that$\lambda_1=\lambda_2=…=\lambda_k=\lambda:\text{Cov}(X_i,X_k)=\lambda^2,i\ne j$ . The residual cross covariances must also be the same, at some $\Delta$, as seen above. (By this point the practical constraint is evident, but will be discussed later, elsewhere. - Sakari) Now we have that the S-LMI imposed cross covariance is

$$
\Lambda\Lambda^T \delta_{\Delta}+\Omega_\Delta= cJ+dI\in span(J,I)
$$

where $\delta\in\mathbb{R}$ is the regression coefficient for subsequent $\eta_1,\eta_2$ and the respective product is for multiple subsequent regressions. $\delta_{\Delta}=\prod_{i=t}^{T}\delta_i$ is the product of regression coefficients of the latent variable to itself at previous timepoints. $\Omega_\Delta$ is a diagonal matrix of residual covariances, all of which are the same.

#### General criterion for indistinguishability (Left out, can be disc

Assumptions

$$
\begin{aligned}
|\lambda_A|&<1\Rightarrow \det A <1 & \text{(VAR is stationary.)} \\
\det\Omega, \det\Psi&\ne0 & \text{(Diagonal residuals and innovations are non-zero.)} \\
\text{rank }\Lambda\Lambda^T&=1& \text{(One common factor.)}
\end{aligned}
$$

We have a set of conditions, and their implications, which must be satisfied, if indistinguishability holds:

$$
\begin{aligned}
\Sigma&=A\Sigma A^T+\Psi &\text{(Within time point VAR covariance is common factor decomposable.)} \\
=\Lambda\Lambda^T+\Omega& \\
=A(\Lambda\Lambda^T+\Omega)A^T + \Psi& \\ 
 \\
\Sigma_{\Delta}&=A^{\Delta}\Sigma &\text{(VAR cross-covariance is LMI decomposable.)} \\\ 
=\Lambda\Lambda^T\delta_{\Delta}+\Omega_{\Delta}& \\ 
&= A^{\Delta}(\Lambda\Lambda^T+\Omega) & \\ 
\forall\Delta & \in1,2,3,... & \\ 
\forall \delta_{\Delta}&\in\mathbb{R} & & \\ 
  \\
\Sigma_{\Delta}^T    &=(\Lambda\Lambda^T\delta_{\Delta})^T+\Omega_{\Delta}^T   &\text{(Cross-covariance is always symmetric.)} \\
= \Lambda\Lambda^T\delta_{\Delta}+\Omega_{\Delta} & \\ 
= \Sigma_{\Delta} & \\ 
 \\
\Sigma_{\Delta} & =A\Sigma_{\Delta-1} & \\
 =(A\Sigma_{\Delta-1})^T & \\
 =\Sigma_{\Delta-1}A^T &
\end{aligned}  
$$

Another, more general, criterion for indistinguishability that the subspace of positive semi-definite symmetric matrices $(\Sigma=\Lambda\Lambda^T+\Omega)\in\mathbb{R}^{K\times K}$ must be invariant under $A$. This is because $\Lambda\Lambda^T\delta + \Omega_{\Delta}$ is always positive semi-definite symmetric.
