---
title: "Theoretical study Quarto -document"
format: pdf
date: "2026-01-28"
editor: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### To do 22.1.

-   Add/integrate previous Rmd file of theoretical result as extension
    to this one.

-   Recheck delta t.

-   Check if subindex VAR(1) is necessary for Sigma.

-   Integrating Tom's notes (26.1 onwards)

### Weak sense stationary VAR(1) and strict longitudinal measurement invariance

In the following VAR(1) process and strict longitudinal measurement
invariance (s-LMI) are compared to each other by the covariance
structure that VAR(1) imposes and how an s-LMI model fits to this (here
equivalently 'is compatible' with) covariance. s-LMI makes sense as a
theoretical model since it captures the simplest scenario of a true
common factor model where only the common factor itself can change.
VAR(1) is also a simple, if not the simplest, vector autoregression
symptom-network model. CT-VAR and VAR can be linked through a
transformation when fixed time intervals are used, or separate study for
CT-VAR can be done.

Especially noteworthy results from mathematical analysis could be
possible contradictions that arise and how these contradictions guide
empirical simulations and tell us the reason why s-LMI might not be
compatible with VAR(1) generated data in the first place. Alternatively,
if some VAR(1) models can generate s-LMI compatible data, what
constraints are necessary for the VAR(1) process to produce it?

First VAR(1) imposed covariance is derived. Then s-LMI imposed
covariance. Then we move on to inspect how they compare by equating them
together to observe possible contradictions or restrictions. We begin
with the simplest and possibly most common scenario where two subsequent
measurement time points (from hereon simply time points) of symptoms are
observed.

Before the analysis, a brief theoretical consideration. We will assume
(*as Jaakko suggested*) that the symptom-network represented as the
VAR(1) model is a state. That is, while stationarity tells us that the
VAR(1) retains its first and second order moments, we will assume that
there is no external cause, which would make systematic changes
('shocks') to the symptoms. These changes could, if included, be
represented as correlated innovations. This assumption then gives us
theoretical circumstance so that the innovations of the VAR(1) are
independent of each other. While this assumption (that no life-events
affect the symptom-network) is unlikely to be true in a real-world
scenario especially when considering longer periods of time (e.g.,
weeks, months), in any case theoretical analysis would become near
impossible if no restrictions of this kind are made.

#### VAR(1) covariance structure at two subsequent time points

The VAR(1) model is defined in matrix format as
$X_{t}=C+AX_{t-1}+\Gamma_t$, where $\Gamma_t$ is independent error
column vector with $E[\Gamma_t]=0$, $C$ is a constant assumed zero. Also
assume centered $X$, $E[X_t]=0$, in our case. Centering makes covariance
calculations easier as the products of expected values can be mostly
ignored (they become 0). $A$ is $K \times K$ (borrowing from CT-VAR
terminology) 'drift' matrix that includes all lagged effects of
$K\times1$ column vectors $X_t$ to $X_{t-1}$, $K$ being the number of
observed items (symptoms). In this section the focus is on the
$2K\times2K$ covariance matrix where two subsequent measurement time
points are observed. All matrices used are real-valued.

First, the covariance matrix (assumed stationary over time) is

$$
\begin{align*}
\text{Cov}(X_t) &= E[X_tX_t^T] \\
&= E[(AX_{t-1}+\Gamma_t)(AX_{t-1}+\Gamma_t)^T] \\
&= E[AX_{t-1}X_{t-1}^TA^T] + \underbrace{E[\Gamma_t \Gamma_t^T]}_{=: \Psi} \\
&= AE[X_{t-1}X_{t-1}^T]A^T + \Psi \\
&= \Sigma_t=:\Sigma_{VAR(1)}
\end{align*}
$$ where stationarity poses that $\Sigma$ is not dependent of time and
so the covariance of VAR(1) is denoted as above from hereon. We will use
$\Sigma_{VAR(1)}$ when we wish to be explicit about meaning the VAR(1)
imposed within time point covariance. $\Gamma_t$ is a random $K\times1$
column vector of (serially) independent innovations at time point $t$,
with \$E\[\\Gamma\]=0,\\text{Var}(\\Gamma=1)\$. $\Psi$ is covariance of
the innovations within time point $t$ - i.e., the contemporaneous
covariance. We will assume that the innovations are independent, and so
$\Psi$ is diagonal.

The vectorized covariance matrix can be solved to equal\
$$
\begin{align*}
\text{vec}(E[X_tX_t^T])&=\text{vec}(\Sigma_{VAR(1)}) \\
&=\text{vec}(AE[X_{t-1}X_{t-1}^T]A^T + \Psi) \\
&=\text{vec}(AE[X_{t-1}X_{t-1}^T]A^T) + \text{vec}(\Psi) \\
&=\text{vec}(A\otimes A)E[X_{t-1}X_{t-1}^T]+\text{vec}(\Psi) \\
\Longrightarrow\\
\text{vec}(E[X_tX_t^T])&=\text{vec}(A\otimes A)\text{vec}(E[X_{t-1}X_{t-1}^T])+\text{vec}(\Psi)&\Rightarrow\\
\text{vec}(E[X_tX_t^T])&=\text{vec}(A\otimes A)\text{vec}(E[X_{t-1}X_{t-1}^T])+\text{vec}(\Psi)&\Rightarrow\\
I&=\text{vec}(A\otimes A)+\text{vec}(\Psi)\text{vec}(E[X_tX_t^T])^{-1}&\Rightarrow\\
I-\text{vec}(A\otimes A) &= \text{vec}(\Psi)\text{vec}(E[X_tX_t^T])^{-1}&\Rightarrow\\
\text{vec}(E[X_tX_t^T])&= (I-A \otimes A)^{-1} \text{vec}(\Psi)
\end{align*}
$$

Where vec is the vectorization operator and $\otimes$ is the Kronecker
product. In the above the mixed Kronecker matrix vector product is used
to obtain the result. Using that we assumed the innovations to be
independent of each other we can rewrite the right hand side so that

$$
\begin{align*}
\text{vec}(\Psi)&=(\psi_{[1,1]},\psi_{[1,2]},...,\psi_{[2,2]},\psi_{[2,3]},...,\psi_{[K,K]})
\\
&=(\psi_{[1,1]},0,...,\psi_{[2,2]},0,...,\psi_{[K,K]}),\\
\end{align*}
$$ and $I-A \otimes A$ is $$
\begin{align*}
\begin{array}
&&1-a_{1,1}a_{1,1}&a_{1,1}a_{1,2}&...&a_{1,1}a_{1,K}&...&...&a_{1,K}a_{1,1}&a_{1,K}a_{1,2}&...&a_{1,K}a_{1,K}&\\
&a_{1,1}a_{2,1}&1-a_{1,1}a_{2,2}&...&a_{1,1}a_{2,K}&...&...&a_{1,K}a_{2,1}&a_{2,K}a_{2,2}&...&a_{2,K}a_{2,K}&\\
&\vdots&\vdots&\ddots&\vdots&&&\vdots&\vdots&\ddots&\vdots&\\
&a_{1,1}a_{K,1}&a_{1,1}a_{K,2}&...&1-a_{1,1}a_{K,K}&...&...&a_{1,K}a_{K,1}&a_{2,K}a_{K,2}&...&a_{2,K}a_{K,K}&\\
&\vdots&\vdots&&\vdots&\ddots&&\vdots&\vdots&&\vdots&\\
&\vdots&\vdots&&\vdots&&\ddots&\vdots&\vdots&&\vdots&\\
&a_{K,1}a_{1,1}&a_{K,1}a_{1,2}&...&a_{K,1}a_{1,K}&...&...&1-a_{K,K}a_{1,1}&a_{K,K}a_{1,2}&...&a_{K,K}a_{1,K}&\\
&a_{K,1}a_{2,1}&a_{K,1}a_{2,2}&...&a_{K,1}a_{2,K}&...&...&a_{K,K}a_{2,1}&1-a_{K,K}a_{2,2}&...&a_{K,K}a_{2,K}&\\
&\vdots&\vdots&\ddots&\vdots&&&\vdots&\vdots&\ddots&\vdots&\\
&a_{K,1}a_{K,1}&a_{K,1}a_{K,2}&...&a_{K,1}a_{K,K}&...&...&a_{K,K}a_{K,1}&a_{K,K}a_{K,2}&...&1-a_{K,K}a_{K,K}&
\end{array}
\end{align*}
$$

Let $c_{[i,j]}$ be and element of $\Sigma$. $\text{vec}(\Sigma)$ then is
$(c_{[1,1]},c_{[2,1]},...,c_{[K,1]},c_{[1,2]},...,c_{[K,2]},...,c_{[K,K]})$.
$(I-A\otimes A) \text{vec}(\Sigma)$ then becomes

##### the above might be omitted, since it maybe is not going anywhere. It does lead to an equation for each innovations gamma_i,j, which might be not relevant.

We will equate the VAR(1) posed $\Sigma_{VAR(1)}$ to s-LMI imposed
covariance further down below.

Second, VAR(1) poses that observations at the time points $X_t, X_{t-1}$
have covariance $$
\begin{align*}
\text{Cov}(X_t,X_{t-1})&=
E[X_tX_{t-1}^T]-E[X_t]E[X_{t-1}]\\&=
E[(AX_{t-1}+\Gamma_t)X_{t-1}^T]\\&=
E[AX_{t-1}X_{t-1}^T]+E[\Gamma_tX_{t-1}^T]\\&=
A\Sigma_{t-1}+E[\Gamma_tX_{t-1}^T]
\end{align*}
$$ Independent errors means that
$E[\Gamma_tX_{t-1}^T]=Cov(\Gamma_t,X_{t-1})=0$ leading to $$
\begin{align*}
\text{Cov}(X_t,X_{t-1})&= A\Sigma_{t-1}\\
\text{Cov}(X_{t-1},X_t)&= \Sigma_{t-1}^T A^T \\
\end{align*}
$$ Where the two covariances above must be the same - i.e., the
covariance matrix is symmetric. This means that every VAR(1) process
implies that covariance of observations from two subsequent time points
$t, t-1$ is $$
Cov((X_{t-1},X_{t}),(X_{t-1},X_{t})) = 
\begin{pmatrix} 
  \Sigma_{t-1} & A\Sigma_{t-1} \\
  \Sigma_{t-1}^TA^T & \Sigma_t
\end{pmatrix} $$ Where the above covariance matrix is the $2K\times 2K$
covariance matrix of the observed data from the two time points. In
addition, stationarity directly implies $\Sigma_t=\Sigma_{t-1}$. (For
now, notation with sub-index *i* will be kept for clarity as it is.
s-LMI is not necessarily stationary, so confusion might be avoided.)

#### s-LMI covariance structure at two subsequent time points

s-LMI with 1 common factor decomposes $\Sigma_{t-1}$ into following
$$\Sigma_{t-1}=\Lambda\Lambda^T+\Omega_{t-1}$$where by definition of
s-LMI $\Omega_{t-1}$ is diagonal and and $\Lambda$ is a $K\times1$
column vector of factor loadings constant over time. We also need the
covariance of the common factor at both time points. Let $\delta$ be the
latent regression coefficient which links the common factor to itself at
a previous time point such that $\eta_t=\delta\eta_{t-1}+\psi_t$, where
$\psi_t$ is independent random term ('innovation', 'error',
'disturbance') with $E[\psi_t]=0$. Assuming standardized common factor
such that $E[\eta_{t-1}]=0,\:Var(\eta_{t-1})=1$ covariance of the common
factor at two subsequent time points is

$$
\begin{align*} \text{Cov}(\eta_{t-1},\eta_t)= E[\eta_{t-1}\eta_t]-E[\eta_{t-1}]E[\eta_t]&=\\ E[\eta_{t-1}(\delta\eta_{t-1}+\psi_t)]&=\\ E[\delta\eta_{t-1}^2+\eta_{t-1}\psi_t]&=\\ \delta Var(\eta_{t-1})&= \delta \end{align*}
$$ Now lets look at the $2K\times 2K$ covariance matrix from the
perspective of strict LMI. A s-LMI model imposes that $$
\text{Cov}((X_{t},X_{t-1}),(X_{t},X_{t-1}))= \begin{pmatrix}    \Lambda & 0
\\   0 & \Lambda \end{pmatrix}  \begin{pmatrix}   1 & \delta 
\\   \delta & \delta+Var(\psi_t) \end{pmatrix} \begin{pmatrix}    \Lambda^T & 0
\\   0 & \Lambda^T \end{pmatrix} + \begin{pmatrix}   \Omega_{t-1} & \Omega_{across} 
\\   \Omega_{across}^T & \Omega_t \end{pmatrix} 
$$

where

$$\begin{pmatrix}    \Lambda & 0\\   0 & \Lambda \end{pmatrix}$$ is a
block matrix that sandwiches the $2\times2$ covariance matrix of the
common factor at both time points.

$$
\begin{align*}&\begin{pmatrix}    \Lambda & 0\\   0 & \Lambda \end{pmatrix} \begin{pmatrix}   1 & \delta \\   \delta & \delta+Var(\psi_t) \end{pmatrix} \begin{pmatrix}    \Lambda^T & 0\\   0 & \Lambda^T \end{pmatrix}+ \begin{pmatrix}   \Omega_{t-1} & \Omega_{across} \\   \Omega_{across}^T & \Omega_t \end{pmatrix} =\\
&\begin{pmatrix}    \Lambda & 0\\   0 & \Lambda \end{pmatrix} \begin{pmatrix}    \Lambda^T & \delta\Lambda^T\\   \Lambda^T\delta & (\delta+Var(\psi_t)\Lambda^T \end{pmatrix} + 
\begin{pmatrix}   
\Omega_{t-1} & \Omega_{across} \\   
\Omega_{across}^T & \Omega_t \end{pmatrix}=\\
& \begin{pmatrix}    
\Lambda\Lambda^T + \Omega_{t-1}& \Lambda\Lambda^T\delta + \Omega_{across}\\   
\Lambda\Lambda^T\delta + \Omega_{across}^T & \Lambda\Lambda^T(\delta+Var(\psi_t)) + \Omega_t 
\end{pmatrix} 
\end{align*}
$$ From the above we see that the strict LMI can only be compatible with
any process with stationary covariance, if
$\delta+Var(\psi_t)=1\Rightarrow1-\delta=Var(\psi_t)$ (assuming
$\Lambda$ is non-zero). (When fitting a s-LMI model this is allowed.) We
also see that s-LMI is compatible with non-stationary processes where
the covariance is proportional to $\delta+Var(\psi_t)$ aligning with
previous theoretical \[Note: insert Tom's analysis\] analysis where
covariance increased over time in a LMI preserving model.

A brief note on notation: We'll be using simply $\Omega$ for the s-LMI
residual covariance, since residual covariance is assumed invariant over
time $\Omega_{t-1}=\Omega_{t}=\Omega_{t+1}=â€¦=\Omega$ .

Using the above auxiliary results we can move to analyse the null
hypothesis (hypotheses) of no difference between VAR(1) and s-LMI.

#### Working null hypothesis (1): If covariance matrix generated by a true VAR(1) model at some (measurement) time point is perfectly explained by a common factor model, then s-LMI model fits perfectly.

Considering only the subset of VAR(1) processes which create a
covariance matrix that can be perfectly explained by a common factor
model is done because we're interested in how (if at all) VAR(1) can
deviate from s-LMI in terms of produced data. Understandably, if any
VAR(1) model creates covariance structure incompatible with s-LMI model
at some time point (i.e., a covariance matrix non-compatible with a
common factor model), then deviation must occur (although the extent to
which this occurs is not clear at this point).

If the VAR(1) generated $2K\times2K$ matrix cannot be explained by the
strict LMI model, this seems likely to be because the off diagonal
blocks of covariance matrices across time points are non-compatible with
the respective s-LMI model imposed across time covariance. Combined with
the restriction on the within time point covariance, this might lead to
contradictions.

This gives us the following null hypothesis (1) equations (from the
$2K\times2K$ matrices imposed by VAR(1) and s-LMI)

$$
\begin{align*}
(I-A \otimes A)^{-1} \text{vec}(\Psi) &= \text{vec}(\Lambda \Lambda^T + \Omega)&&\Rightarrow
\\
(I-A \otimes A)^{-1} \text{vec}(\Psi) &= \text{vec}(\Lambda \Lambda^T) + \text{vec}(\Omega)
\\
AE[X_{t-1}X_{t-1}^T]A^T + \Psi&=\Lambda \Lambda^T + \Omega\Rightarrow
\\
A\Sigma A^T+\Psi&=\Sigma\Rightarrow\\
(A\otimes A)\text{vec}(\Sigma)&=\text{vec}(\Sigma)-\text{vec}(\Psi)
\end{align*}
$$

and

$$
\begin{align*}
&A\Sigma_{t-1}=\Lambda\Lambda^T\delta+\Omega_{across}
\end{align*}
$$

both of which must be true for the null hypothesis (1) to hold. Assuming
that the null hypothesis (1) is true, further analysis of the respective
equations show

$$
\begin{aligned}
A\Sigma_{t-1} &= \Lambda\Lambda^T\delta+\Omega_{cross} \Leftrightarrow\\
A &= \Lambda\Lambda^T\delta \Sigma_{t-1}^{-1} + \Omega_{cross} \Sigma_{t-1}^{-1} \Leftrightarrow\\
A + \delta \Omega \Sigma_{t-1}^{-1} &= \Lambda\Lambda^T\delta \Sigma_{t-1}^{-1} + \delta \Omega \Sigma_{t-1}^{-1} + \Omega_{cross} \Sigma_{t-1}^{-1} \Leftrightarrow\\
A + \delta \Omega \Sigma_{t-1}^{-1} &= \delta \underbrace{(\Lambda\Lambda^T + \Omega)}_{=\Sigma_{t-1} \text{ by assumption}} \Sigma_{t-1}^{-1} + \Omega_{cross} \Sigma_{t-1}^{-1} \Leftrightarrow\\
A &= \delta I + (\Omega_{cross} - \delta \Omega)\Sigma_{t-1}^{-1}.
\end{aligned}
$$

From the above equation we see that $A$ must NOT NECESSARILY be
symmetric under the null hypothesis (1) because both
$\Sigma,\Sigma_{across}$ are covariance matrices (or a precision matrix)
and otherwise only re-scaling with a scalar, multiplying by a diagonal
matrix (since diagonal matrix commutes with all matrices) is done for
symmetric matrices.

Combining, we get a system of (matrix) equations

$$
\begin{cases}
&(I-A \otimes A)^{-1} \text{vec}(\Psi) &=& \text{vec}(\Lambda \Lambda^T + \Omega)\\
&A &=& \delta I + (\Omega_{cross} - \delta \Omega)\Sigma_{t-1}^{-1}
\end{cases}
$$

substituting $A$ into the upper equation and further substituting
$A-\delta I$

$$
\begin{align*}
(I-(\delta I + (\Omega_{cross} - \delta \Omega)\Sigma_{t-1}^{-1}) \otimes (\delta I + (\Omega_{cross} - \delta \Omega)\Sigma_{t-1}^{-1}))^{-1} \text{vec}(\Psi) &= 
\\  (I-(\delta I + (\Omega_{cross} - \delta \Omega)\Sigma_{t-1}^{-1}) \otimes\delta I \\+ (\delta I + (\Omega_{cross} - \delta \Omega)\Sigma_{t-1}^{-1})\otimes ((\Omega_{cross} - \delta \Omega)\Sigma_{t-1}^{-1}))^{-1} \text{vec}(\Psi)&=
\\
(I-\delta I\otimes \delta I + (\Omega_{cross} - \delta \Omega)\Sigma_{t-1}^{-1} \otimes\delta I + \delta I\otimes(\Omega_{cross} - \delta \Omega)\Sigma_{t-1}^{-1}
\\+  
(\Omega_{cross} - \delta \Omega)\Sigma_{t-1}^{-1}\otimes (\Omega_{cross} - \delta \Omega)\Sigma_{t-1}^{-1})^{-1} \text{vec}(\Psi)&=
\\
(I-\delta^2I+(\Omega_{cross} - \delta \Omega)\Sigma_{t-1}^{-1} \otimes\delta I + \delta I\otimes(\Omega_{cross} - \delta \Omega)\Sigma_{t-1}^{-1}
\\+(\Omega_{cross} - \delta \Omega)\Sigma_{t-1}^{-1}\otimes (\Omega_{cross} - \delta \Omega)\Sigma_{t-1}^{-1})^{-1} \text{vec}(\Psi)&=
\\
(I-\delta^2I+(A- \delta I) \otimes\delta I + \delta I\otimes(A- \delta I)
\\+(A- \delta I)\otimes (A- \delta I))^{-1} \text{vec}(\Psi)&=
\\
(I-\delta^2I + A\otimes \delta I - \delta^2I + \delta I\otimes A + \delta^2I
\\+ (A- \delta I)\otimes A + (A- \delta I)\otimes (- \delta I))^{-1} \text{vec}(\Psi)&=
\\
(I-2\delta^2I+ \delta^2I + A\otimes \delta I + \delta I\otimes A 
\\+ A\otimes A - \delta I\otimes A + A\otimes(- \delta I) - \delta I\otimes (- \delta I))^{-1} \text{vec}(\Psi)&=
\\
(I-2\delta^2I+ \delta^2I  
+ A\otimes A   - \delta I\otimes (- \delta I))^{-1} \text{vec}(\Psi)&=
\\
(I-2\delta^2I+ 2\delta^2I  
+ A\otimes A  )^{-1} \text{vec}(\Psi)&=
(I-A\otimes A  )^{-1} \text{vec}(\Psi)
\end{align*}
$$

The analysis leads to no easy contradiction. (Chat GPT 3.5 confirmed
that the above analysis is correct. Hence, we're uncertain if it is
correct.) We'll see if contradictions arise when including three time
points in the analysis below. But first, we need to do some
generalizations and discuss the implications of VAR(1) model and the
null hypothesis (1) further.

#### VAR(1) covariance compared to s-LMI covariance at T time points.

As the scenario where two subsequent measurement points are observed is
possibly the most common one, we'll keep the main null hypothesis (1) as
the respective scenario. On the other hand, it is of interest to analyze
what happens when multiple subsequent time points are included. This is
perhaps less common in measurement invariance literature, but more
common in VAR literature.

We have the assumption (proven below) that as the distance in time
between two time points $\Delta t=(t_i-t_j)\to\infty, \:i>j$ samples
from VAR(1) model generated data at those two time points have 0
covariance. This would make the asymptotic $2K\times2K$ perfectly
explained by a measurement invariance model, because the main diagonal
covariance matrices $\Sigma_{t-1}=\Sigma_t$ are perfectly explained by a
common factor model by assumption, and the off diagonal matrices are 0,
which is allowed in a strict LMI model (no cross-covariance between the
observed time points and 0 regression coefficient for the latent
variable). Again, we are assuming that VAR(1) model generated $\Sigma$
perfectly compatible with s-LMI. This is a less practically meaningful
case arguably since observed data with 0 across time point covariance is
not common. This also does prove that a VAR(1) process could in some
sense be the true data generating process even if no across time point
covariance is observed (no lagged effects are estimated) if one were to
claim that $\Delta t$ is very small: We're just not observing time
points close enough to each other to see the VAR(1).

Nevertheless - considering that the above scenario is not a typical one
in psychopathology research - we can attempt to generalize the above
results concerning the $2K\times2K$ matrix to an $TK\times TK$ matrix
where we have $T$ measurement of $K$ symptoms over occasions at constant
time intervals. Brief notes are made as we move on and a summary at the
end.

Here we change the notation a little and use an arbitrary time point $t$
as the first measurement time point, increasing ${t, t+1,...,T}$. The
$TK\times TK$ matrix is

$$
\begin{array}
  \\\Sigma_t&...&\Sigma_{t,T}
  \\ \vdots&\ddots&\vdots
  \\ \Sigma_{t,T}^T&...&\Sigma_T
\end{array}
$$

Proceeding again from VAR(1) to s-LMI and then equating between the
models. Let $^{(T)}$ denote the matrix raised to power of $T$. The
$\Sigma_{t,T}$ for VAR(1) is

$$
\Sigma_{t,T}=\text{Cov}(X_t, X_{T}) = A^{(T)}\Sigma_{VAR(1)}
$$

*Proof of the asymptotic 0 across time point covariance*. On a brief
note we can further decompose the above equation, using the power method
of eigenvalues, into

$$
PD^{(T)}P^{-1}\Sigma_{VAR(1)}
$$

where $D$ is a diagonal matrix of eigenvalues of $A$, $P$ is and
orthonormal matrix of eigenvectors of $A$ as columns. From this
decomposition we directly see the above mentioned asymptotic property
that as distance in time between time points increases, $D$ is raised to
a larger power and decreases eventually to the zero matrix. This is
because eigenvalues of (stationary) $A$ are less than one, meaning that
all diagonal elements of $\text{diag}(D):|d_{ii}|<1$ as well, and hence
the matrix power converges to $0$. This means that a true data
generating stationary VAR(1) model should produce across time point
covariances that reduce to zero as distance in time between any two time
points increases (practically speaking perhaps as years pass). Such
observations might be sparse in psychopathology literature and it is
accepted in the literature that even if any autoregressive model would
be the true data generating model, it would unlikely be stationary over
lengthier time periods (e.g., years again). This property of stationary
VAR(1) does - as we have -- (*as Jaakko suggested*) restrain the
analysis to a psychopathological state, but this contemplation is
omitted here.

For s-LMI, respectively, we'll use
$\delta_{2}, \delta_{3}, ..., \delta_t,\delta_{t+1},...,\delta_{T}$ to
denote the regression coefficient between subsequent time points (there
is no regression at the first time point in s-LMI). Also, the
$\Omega_{t,t+1}$ needs to be generalized to include residual covariances
between any two time points so that $\Omega_{t,T}$ is the residual
covariance between time point $t$ and time point $T$. $\Omega$ is the
within time point residual covariance invariant over time.

Few more generalizations and constraints before we can equate the VAR(1)
implied covariance to the s-LMI implied covariance again using multiple
time points. We have that the VAR(1) imposed covariance between any two
subsequent time points is the same. This also generalizes to the VAR(1)
imposed covariance between any two equidistant time points as they are
$A^{\Delta t}\Sigma$, which only depends on the distance in time. This
means that $\delta_t$ must be a constant since otherwise the s-LMI
imposed covariance between two subsequent time points
$\Lambda\Lambda^T\delta + \Omega_{t,t+1}$ would not be the same. More
precisely, $\Omega_{t,t+1}$ is by definition diagonal and so can only
change the diagonal to some extent - off-diagonal elements would not be
the same. This also means that $\Omega_{t,t+1}$ is the same for any $t$.

Using that $\delta$ is constant (and other assumptions established
above), the covariance between the common factor to itself between any
time points two time intervals apart from each other is

$$
\begin{align*}
\text{Cov}(\eta_t,\eta_{t+2})&=E[\eta_{t}\eta_{t+2}]\\
&=E[\eta_{t}  (\delta_{t+2}\eta_{t+1}+\psi_{t+2})  ]\\
&=E[\eta_{t}  (\delta_{t+2}(\delta_{t+1}\eta_{t}+\psi_{t+1})+\psi_{t+2})  ]\\
&=E[\eta_{t}  (\delta_{t+2}\delta_{t+1}\eta_{t}+\delta_{t+2}\psi_{t+1}+\psi_{t+2})  ]\\
&=E[\delta_{t+2}\delta_{t+1}\eta_{t}\eta_{t}]+E[\delta_{t+2}\psi_{t+1}\eta_{t}]+E[\psi_{t+2}\eta_{t}]\\
&=\delta_{t+2}\delta_{t+1}=\delta^2
\end{align*}
$$

and for three time intervals apart

$$
\begin{align*}
\text{Cov}(\eta_t,\eta_{t+3})&=E[\eta_{t}\eta_{t+3}]\\
&=E[\eta_{t}  (\delta\eta_{t+2}+\psi_{t+3})  ]\\
&=E[\eta_{t}  (\delta(\delta\eta_{t+1}+\psi_{t+2})+\psi_{t+3})  ]\\
&=E[\eta_{t}  (\delta(\delta(\delta\eta_{t}+\psi_t)+\psi_{t+2})+\psi_{t+3})  ]\\
&=E[\eta_{t}  (\delta(\delta^2\eta_{t}+\delta\psi_t)+\psi_{t+2})+\psi_{t+3})  ]\\
&=E[\eta_{t}  (\delta^3\eta_{t}+\delta^2\psi_t+\delta\psi_{t+2}+\psi_{t+3})  ]\\
&=E[\eta_{t}\delta^3\eta_{t}+\eta_{t}\delta^2\psi_t+\eta_{t}\delta\psi_{t+2}+\eta_{t}\psi_{t+3})  ]\\
&=\delta^3=\text{Cov}(\eta_t,\eta_{t+2})\delta
\end{align*}
$$

Implying that

$$
\begin{align*}
\text{Cov}((X_{t},X_{t+3}),(X_{t},X_{t+3}))
&= 
\begin{pmatrix}    \Lambda & 0
\\   0 & \Lambda \end{pmatrix}  \begin{pmatrix}   1 & \delta^2 
\\   \delta^2& \delta^2+Var(\psi_{t+3}) \end{pmatrix} \begin{pmatrix}    \Lambda^T & 0
\\   0 & \Lambda^T \end{pmatrix} + \begin{pmatrix}   \Omega & \Omega_{t,t+3} 
\\   \Omega_{t,t+3}^T & \Omega 
\end{pmatrix} 
\\
&=
\begin{pmatrix}    
 \Lambda\Lambda^T + \Omega & \Lambda\Lambda^T\delta^2 + \Omega_{t,t+3} \\   
 \Lambda\Lambda^T\delta^2 + \Omega_{t,t+3}^T & \Lambda\Lambda^T(\delta^2+Var(\psi_{t+3})) + \Omega 
\end{pmatrix} 
\end{align*}
$$

##### the above equations could be proven by induction. Other route maybe is to show that the ratio between covariances between two time points of distance delta_t and delta_t+1 is constant. This is currently omitted.

To summarize $$
\begin{align*}
\forall\Delta t:\text{Cov}(X_t,X_{t+\Delta t})&=A^{\Delta t}\Sigma&&\Rightarrow
\\
\delta_1=\delta_2=...&=\delta
\\
\Omega_{t,t+1}=\Omega_{t,t+2}=...&=\Omega_{\Delta t}
\end{align*}
$$ must be true for s-LMI to be compatible with VAR(1) generated data.
And this implies that the covariance between any two time points must be
$\Lambda\Lambda^T\delta^{\Delta t} + \Omega_{t,T}$.

From the previous result for the $2K\times2K$ matrix we can then
generalize to the $TK\times TK$ matrix

$$
\begin{array}
\\\Lambda\Lambda^T + \Omega & ... &  \Lambda\Lambda^T \delta^{(T-1)} + \Omega_{1,T}
\\ \vdots&\ddots&\vdots
\\ \Lambda\Lambda^T \delta^{(T-1)} + \Omega_{1,T}^T&...&\Lambda\Lambda^T(\delta+Var(\psi_t)) + \Omega
\end{array}
$$

Now we can obtain the more general, auxiliary null hypothesis equation
(A1) equations relating the across time point covariances and within
time point covariances for any lag

$$
\begin{align*}
\Sigma_{\Delta t}&=\\
A^{(\Delta t)}\Sigma&=\Lambda\Lambda^T \delta^{\Delta t}+ \Omega_{\Delta t}&&\Rightarrow\\
A^{(\Delta t)}&=(\Lambda\Lambda^T \delta^{\Delta t} + \Omega_{\Delta t})\Sigma^{-1}
\end{align*}
$$

which is NOT NECESSARILY symmetric, as was the case for the null
hypothesis equation (1). (The above is true for all $\Delta t$.)

Now, equating the VAR(1) imposed covariances and s-LMI imposed
covariances, as before, using three subsequent time points we have

$$
\begin{cases}
&(I-A \otimes A)^{-1} \text{vec}(\Psi) &=& \text{vec}(\Lambda \Lambda^T + \Omega)\\
&A &=& \delta I + (\Omega_{1,2} - \delta \Omega)\Sigma_{}^{-1}\\
&A^{(2)}&=&\delta^2 I + (\Omega_{1,3} - \delta^2 \Omega)\Sigma_{}^{-1}
\end{cases}
$$

hence

$$
\begin{align*}
A^{(2)}&=(\delta I + (\Omega_{1,2} - \delta \Omega)\Sigma_{}^{-1})(\delta I + (\Omega_{1,2} - \delta \Omega)\Sigma_{}^{-1})\\
&=\delta^2I+\delta I(\Omega_{1,2} - \delta \Omega)\Sigma_{}^{-1}+(\Omega_{1,2} - \delta \Omega)\Sigma_{}^{-1}\delta I+(\Omega_{1,2} - \delta \Omega)\Sigma_{}^{-1}(\Omega_{1,2} - \delta \Omega)\Sigma_{}^{-1}\\
&=\delta^2I+\delta I(\Omega_{1,2} - \delta \Omega)\Sigma_{}^{-1}+(\Omega_{1,2} - \delta \Omega)\Sigma_{}^{-1}\delta I+(\Omega_{1,2} - \delta \Omega)\Sigma_{}^{-1}(\Omega_{1,2} - \delta \Omega)\Sigma_{}^{-1}\\
\end{align*}
$$

### Summary and implications OLD 31.1.2024

-   A direct observation from the null hypothesis (1) and auxiliary null
    hypothesis (A1) equations shown above is that $A$ must always be
    symmetric for the common factor s-LMI model to be compatible with
    stationary VAR(1) generated data. Hence, asymmetry in $A$ produces
    incompatibility to s-LMI.

-   s-LMI is compatible with the idea that VAR(1) poses that across time
    point covariance approaches 0 as the distance between two time
    points increases. This scenario might not be frequently observed
    however, suggesting that - at least some part of - psychopathology
    processes cannot be understood as vector autoregressive processes.
    This is true for any stationary process to my knowledge, as the
    requirement is that the process will necessarily remain bounded to
    some extent to its 'state'. In fact, it might be reasonable to
    integrate both viewpoints as is done in trait-state models.

### Next

-   Perhaps one possibility is something like Chi-squared testing with
    vectorized (non-redundant) squared elements of
    $A_{lower-tri}^T-A_{upper-tri}$.
-   At this point it still is not evident that VAR(1) can generate a
    covariance matrix perfectly decomposable to a common factor
    model(?). Even more, if this is possible for multiple subsequent
    timepoints is unclear.
-   Also it is not clear what happens when sampling from a population at
    different time points so that subjects are sampled at different time
    lags.
-   Stationarity is also not necessarily a condition which should be
    imposed, which can be discussed further.
-   VAR(1) is a special case of CT-VAR. The respective transformations
    from CT-VAR to VAR(1) are available.
-   Measures of asymmetry such as $s$ $$
    s \equiv (|A_{sym}|-|A_{anti}|)/(|A_{sym}|+|A_{anti}|)
    $$ {#eq-asymmetry} where $A$ is decomposed into its symmetric and
    asymmetric parts can be used also. The metric above is shown at:
    [stack
    exchange](https://math.stackexchange.com/questions/2048817/metric-for-how-symmetric-a-matrix-is).
    Asymmetry of $A$ could further be approached analytically for
    example by decomposing $A$ into its symmetric and asymmetric parts,
    and/or through simulations where asymmetry of $A$ is varied by
    producing random matrices with some logic with how asymmetric is
    produced in $A$.
-   

#### 
